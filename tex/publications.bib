@inproceedings{trustee,
	title        = {AI/ML and Network Security: The Emperor has no Clothes},
	author       = {A. S. Jacobs and \textbf{R. Beltiukov} and W. Willinger and R. A. Ferreira and A. Gupta and L. Z. Granville},
	year         = 2022,
	booktitle    = {ACM CCS},
	address     = {LA, USA}
}

@InProceedings{qfac,
author="\textbf{Beltiukov, R.}",
title="Optimizing Q-Learning with K-FAC Algorithm",
booktitle="Analysis of Images, Social Networks and Texts (Springer)",
year="2020",
address="Cham",
abstract="In this work, we present intermediate results of the application of Kronecker-factored Approximate curvature (K-FAC) algorithm to Q-learning problem. Being more expensive to compute than plain stochastic gradient descent, K-FAC allows the agent to converge a bit faster in terms of epochs compared to Adam on simple reinforcement learning tasks and tend to be more stable and less strict to hyperparameters selection. Considering the latest results we show that DDQN with K-FAC learns more quickly than with other optimizers and improves constantly in contradiction to similar with Adam or RMSProp.",
isbn="978-3-030-39575-9"
}

@inproceedings{beltiukov2023pinot,
  title={PINOT: Programmable Infrastructure for Networking},
  author={\textbf{Beltiukov, Roman} and Chandrasekaran, Sanjay and Gupta, Arpit and Willinger, Walter},
  booktitle={Proceedings of the Applied Networking Research Workshop},
  pages={51--53},
  year={2023}
}

@inproceedings{beltiukov2023search,
  author = {\textbf{Beltiukov, Roman} and Guo, Wenbo and Gupta, Arpit and Willinger, Walter},
  title = {{In Search of NetUnicorn: A Data-Collection Platform to Develop Generalizable ML Models for Network Security Problems}},
  year = {2023},
  isbn = {9798400700507},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3576915.3623075},
  doi = {10.1145/3576915.3623075},
  abstract = {The remarkable success of the use of machine learning-based solutions for network security problems has been impeded by the developed ML models' inability to maintain efficacy when used in different network environments exhibiting different network behaviors. This issue is commonly referred to as the generalizability problem of ML models. The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem. Unfortunately, these methods are generally ill-suited or even counterproductive in the network security domain, where they often result in unrealistic or poor-quality datasets.To address this issue, we propose a new closed-loop ML pipeline that leverages explainable ML tools to guide the network data collection in an iterative fashion. To ensure the data's realism and quality, we require that the new datasets should be endogenously collected in this iterative process, thus advocating for a gradual removal of data-related problems to improve model generalizability. To realize this capability, we develop a data-collection platform, netUnicorn, that takes inspiration from the classic "hourglass'' model and is implemented as its "thin waist" to simplify data collection for different learning problems from diverse network environments. The proposed system decouples data-collection intents from the deployment mechanisms and disaggregates these high-level intents into smaller reusable, self-contained tasks. We demonstrate how netUnicorn simplifies collecting data for different learning problems from multiple network environments and how the proposed iterative data collection improves a model's generalizability.},
  booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
  pages = {2217â€“2231},
  numpages = {15},
  keywords = {artificial intelligence, data collection, network security, generalizability, machine learning},
  location = {Copenhagen, Denmark},
  series = {CCS '23}
}

